# üßê Explicaci√≥n del Proyecto de Modelos de Deep Learning (MLP y LSTM)

Este documento explica de forma clara y sencilla c√≥mo se desarrollaron y evaluaron los dos modelos de Deep Learning del proyecto: **MLP** y **LSTM**, con el fin de predecir la gravedad de un siniestro vial (leve, grave o fatal) en la ciudad de Bogot√°.

---

## üîÑ 1. Flujo general del proyecto

1. Se parte de dos bases de datos diferentes:
   - `Data_SinNull.csv`: versi√≥n completa, sin valores nulos.
   - `Data.csv`: versi√≥n m√°s balanceada, con mejor distribuci√≥n entre clases. (Contiene nulos)

2. Se elige `Data.csv` para el entrenamiento final porque mejora el rendimiento en clases minoritarias. Lo que se realiza es una imputaci√≥n de datos, en donde: 
    - Servicio se dej√≥ los faltante como "Desconocido".
    - Clase de vehiculo se imput√≥ con la categor√≠a m√°s frecuente (moda)
3. Se procesan las variables categ√≥ricas usando codificaci√≥n con **embeddings** mediante `LabelEncoder`.
4. Se dividen los datos en conjuntos de entrenamiento (70%), validaci√≥n (20%) y prueba (10%).
5. Se entrenan dos modelos distintos:
   - Un modelo **MLP** (red neuronal multicapa)
   - Un modelo **LSTM** (red secuencial)
6. Se eval√∫a cada modelo con el conjunto de prueba y se comparan los resultados.

---

## üî¢ 2. Variables utilizadas

- Variables **categ√≥ricas** codificadas con embeddings:
  - CLASE (tipo de accidente - se determin√≥ Choque)
  - TIPO_SERVICIO
  - CLASE_VEHICULO
  - CAUSA_PROBABLE

- Variables **num√©ricas**:
  - A√ëO
  - MES
  - DIA_SEMANA

La variable objetivo (`GRAVEDAD`) fue convertida a valores num√©ricos: 0 = Leve, 1 = Grave, 2 = Fatal.

---

## üîß 3. Modelo MLP (Multilayer Perceptron) ‚Äì Paso a paso

1. Se definen las dimensiones de embeddings para cada variable categ√≥rica.
2. Se codifican las variables categ√≥ricas con `LabelEncoder`.
3. Se construye el modelo:
   - Entradas de cada variable categ√≥rica ‚Üí capa `Embedding` ‚Üí `Flatten`
   - Se concatenan todas las salidas de embeddings con las variables num√©ricas
   - Se pasa por capas densas (`Dense`) con activaci√≥n ReLU y `Dropout`
   - Salida con `softmax` para 3 clases
4. Se entrena el modelo con `sparse_categorical_crossentropy`, `Adam`, y pesos por clase.

**Ventajas:** Simple, r√°pido de entrenar.

**Limitaci√≥n:** Menor sensibilidad a clases desbalanceadas como "Fatal".

---

## üîß 4. Modelo LSTM (Long Short-Term Memory) ‚Äì Paso a paso

1. Se usan los mismos embeddings por variable categ√≥rica.
2. En lugar de concatenar y aplanar directamente, se crea una "secuencia artificial":
   - Cada vector embedding se mantiene como (1, emb_dim)
   - Se concatena como una secuencia de pasos temporales (aunque no lo son en realidad)
3. Esta secuencia se pasa por una capa `LSTM` para aprender combinaciones complejas.
4. La salida del LSTM se concatena con las variables num√©ricas y pasa por capas densas.
5. Se entrena con los mismos par√°metros y estructura de validaci√≥n.

**Ventajas:** Mayor capacidad de generalizaci√≥n y sensibilidad a clases poco frecuentes.

**Limitaci√≥n:** Requiere m√°s tiempo de entrenamiento.

---

## üìä 5. Evaluaci√≥n de modelos

Cada modelo se evalu√≥ con:
- Accuracy (precisi√≥n global)
- F1-Score por clase y macro promedio
- AUC por clase (curvas ROC)

Los resultados fueron almacenados en `resultados/*.json` y comparados autom√°ticamente con el script `04_comparacion_modelos.py`.

---

## üìÖ 6. Conclusi√≥n

- Ambos modelos pueden predecir la gravedad de un siniestro, pero **LSTM ofrece mejor equilibrio** entre clases.
- El MLP puede ser √∫til para sistemas en tiempo real por su velocidad.
- La combinaci√≥n embeddings + Deep Learning fue efectiva para tratar datos categ√≥ricos.
- El uso de una versi√≥n m√°s balanceada de la base (`Data.csv`) permiti√≥ mejorar el rendimiento global.



